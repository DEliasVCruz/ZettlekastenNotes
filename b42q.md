# cli.py.pytest

Command line interface for Python's pytests

## Synopsis

```sh
  pytests [<options>]
```

## Overview

- By **default** pytest will **run** only those **tests** that are **in or
  under** the **current directory**

- You can perform test [paramatrization](./e051.md) that involves running the **same
  test with different inputs**

- I has a **plugin-based architecture**, which allows for customization and easy
  integration with other tools

### Configure pytest

You can configure `pytest` inside your `pyproject.toml`. This is usefull for
**specifying markers**

- `addopts`: **Default options** when running pytests
- `markers`: Registered markers

```toml
# Content of pyproject.toml
  [tool.pytest.ini_options]
  addopts = --strict-markers
  markers = [
      "webtest: mark a test as a webtest.",
      "slow: mark test as slow."
  ]
```

### Flags

- `-k`: Only run test whose **name match an expression**
- `-m`: **Include or exclude** tests from **defined categories** marks
- `-q`: Keep the output brief
- `-lf`: **Run only** the **last failed** tests and ignore the ones that passed
- `-ff`: **Run previously failed tested first** and then run the rest
- `--stepwise`: **Stop at the first failing test** and resume when invoked again
- `--durations=n`: **Report** the **slowest** `n` number of tests
- `--duration-min`: Minimun durationn for dispalying with `--durations`
- `--markers`: List of all the known marks
- `--strict-markers`: **Ensure** all marks in your tests **are registered** in your
  pytest config

### Usefull plugins

- [pytest-randomly](https://github.com/pytest-dev/pytest-randomly): Run your test in a random order
- [pytests-cov](https://pytest-cov.readthedocs.io/en/latest/jkj): Integration with the [coverage](https://coverage.readthedocs.io/) package
- [pytests-bdd](https://pytest-bdd.readthedocs.io/en/latest/): Use Test Driven Development with [Gherkin](http://docs.behat.org/en/v2.5/guides/1.gherkin.html)

## Cookbook

### Running tests

You can run tests from:

- Individual test files
- All tests files in a directory
- A **keyword** expression
  - **Class** names
  - **Function** names

```sh
  pytest test_mod.py
  pytest testing/
  pytest -k "MyClass and not method"
```

The last one will execute `TestMyClass.test_something` but not `TestMyClass.test_method_simple`

### Re-run failed tests

You can instruct `pytest` to run failed test only or first with `-lf` and `-ff`
flags respectively

```sh
  pytests tests_failing.py
  pytest --lf tests_failing.py
  pytest --ff tests_failing.py
```

YOu can also use the `--stepwise` flag to run test and stop when a failed
tested is encounter. On the next invocation it will resume from the last failed
test until the next failing tests

```sh
  pytests tests_failing.py
  pytests --stepwise test_failing.py
# Fix the failing test

  pytests --stepwise test_failing.py
# Fix the next failing test
```

You can also use the `--stepwise-skip` flag to ignore the first failing test
and continue until the next failing one (in case you are stuck and want to move
on)

```sh
  pytests tests_failing.py
  pytests --stepwise test_failing.py
# Try fix the failing test

  pytests --stepwise-skip test_failing.py
# Continue with next failing test
```

### Run tests with hierarchical queries

You can run tests by specifying them in a **hierarchy** from the **file name
to** the tested **function** or **method** separated by `::`

```sh
  pytest test_mod.py::test_func
  pytest test_mod.py::TestClass::test_method
```

### Registering marks

If you have a large number of marks, to avoid miss labeling a mark and keep
them consistent in your test suite you can register them in your pytest
configuration

You can then enforce that consistency by passing the `--strict-markers` when
you run pytests

```sh
  pytest --strict-markers
```

### Run only tests with the specified marks

You can run test based on their categories in marks with the `-m` flag.

```sh
  pytest -m database_access
```

To **run all test excepts** the one that are part of the **specified category** mark
just use `not` before your mark name

```sh
  pytest -m "not database_access"
```

### Display how much time your tests take

You can use the `--durations=` option to display how much your test are taking.
It expects an integer value and it outputs the slowest `n` number of tests

You can also use teh `--duration-min=` option to specify the treshold for
displaying durations

**In this example** it will show the top 3 slowest tests that are over 1.0
seconds long

```sh
  pytests --durations=3 --duration-min=1.0
```
